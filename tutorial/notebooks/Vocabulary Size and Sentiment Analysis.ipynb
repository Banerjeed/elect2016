{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"I know words...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trump said *\"I know words, I have the best words.\"* So we'll be comparing his Tweeter vocabulary size with Clinton's to see if his tweets reflect this statement.\n",
    "\n",
    "In essence, the vocabulary size is simply the number of unique words used in a text. However, we need to make some choices about the words we count. For example,\n",
    "\n",
    "* words like \"a\", \"the\", \"at\", \"on\" don't really reflect vocabulary richness;\n",
    "* singular and plural forms of a noun should only be counted once, since using \"cat\" and \"cats\" does not imply a larger vocabulary than using only \"cat\" or only \"cats\";\n",
    "* similarly, a verb used in multiple tenses (e.g., \"argue\", \"argued\", \"arguing\") should probably only be counted once;\n",
    "* proper nouns should also not be counted as part of the vocabulary, because they don't reflect vocabulary richness;\n",
    "* numbers don't reflect vocabulary size either;\n",
    "* hashtags and usernames used on Twitter should also be excluded from the count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Twitter data tends to be very messy, we must do quite a ~~bit~~ lot of data cleaning. The steps done to clean the data are described below.\n",
    "\n",
    "1. Trump, when tweeting from Android, mostly retweets by copy-pasting other users' tweets. Because these tweets were not originally posted by Trump, they are flagged as copy-pasted and excluded from the analysis.\n",
    "2. Some tweets consist of quotes from articles published by the media. These tweets are also flagged and excluded from the analysis.\n",
    "3. Words separated with an em dash, an en dash, or a forward slash are not handled by NLTK as individual words. Therefore, we will try to identify and separate these words 'manually'.\n",
    "4. Exclude stopwords and other words with less than tree characters (many are abbreviations).\n",
    "5. Exclude hashtags, user mentions, punctuation marks, proper nouns, ordinal and cardinal numbers (written with digits or spelled out)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the paths...\n",
    "os.environ['STANFORD_MODELS'] = \"/Users/gogrean/code/stanford-ner-2014-06-16/classifiers\"\n",
    "os.environ['CLASSPATH'] = \"/Users/gogrean/code/stanford-ner-2014-06-16\"\n",
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pandas to read in the CSV files in which we saved the candidates' tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_dir = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "\n",
    "df_trump = pd.read_csv(root_dir + \"data/pyladies_trumps_tweets.csv\", encoding = \"utf-8\")\n",
    "df_clinton = pd.read_csv(root_dir + \"data/pyladies_clintons_tweets.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the first few rows of the data frames..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_trump.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clinton.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*EXERCISE: Write a function that identifies copy-pasted tweets. Create a new column in the dataframes to flag copy-pasted tweets.*\n",
    "\n",
    "The next cell loads some test cases, while the following one loads a possible solution in case you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load test_copy_paste_cases.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load copy_paste_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*EXERCISE: Write a function that identifies tweets consisting of quoted statements from the media. Create a new column in the dataframes to flag these tweets.*\n",
    "\n",
    "The next cell loads some test cases, while the following one loads a possible solution in case you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load test_article_quote_cases.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load article_quote_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the two functions above, they can be used to create `copy_paste` and `article_quote` columns in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I import my own functions, but feel free to use yours instead.\n",
    "\n",
    "from utils import is_copy_paste, is_article_quote\n",
    "\n",
    "for i, text in enumerate(df_trump.text):\n",
    "    df_trump.at[i, 'copy_paste'] = is_copy_paste(text)\n",
    "    df_trump.at[i, 'article_quote'] = is_article_quote(text)\n",
    "\n",
    "for i, text in enumerate(df_clinton.text):\n",
    "    df_clinton.at[i, 'copy_paste'] = is_copy_paste(text)\n",
    "    df_clinton.at[i, 'article_quote'] = is_article_quote(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the dataframes to exclude copy-pasted tweets and tweets that consist of quoted media statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_tweets_trump = df_trump[(df_trump.copy_paste == False) & (df_trump.article_quote == False)]\n",
    "filtered_tweets_trump.reset_index(inplace=True)\n",
    "\n",
    "filtered_tweets_clinton = df_clinton[(df_clinton.copy_paste == False) & (df_clinton.article_quote == False)]\n",
    "filtered_tweets_clinton.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenize the tweets, and write the functions necessary to calculate the vocabulary size and to do the sentiment analysis. The `Tweets` class below provides most of the functionality. Please go through the code to understand what it's doing. \n",
    "\n",
    "A `Tweets` object is created from a dataframe containing the tweets info we downloaded in the previous notebook, e.g.\n",
    "\n",
    "```\n",
    "tweets = Tweets(filtered_tweets_clinton)\n",
    "```\n",
    "\n",
    "When instantiating `Tweets`, the text of the tweets is automatically tokenized. The tokens are afterwards used by `tag_corpus` to tag the words using the Stanford NER tagger and the default NLTK POS tagger. Using the tagged words, we exclude words that are not counted when calculating the vocabulary size (see explanation at the beginning of this notebook) using the function `filter_voc_words`. The size of the vocabulary is just the number of unique words left after filtering. the unique words are returned by `filter_unique_voc_words`, e.g.\n",
    "\n",
    "```\n",
    "tweets = Tweets(filtered_tweets_clinton)\n",
    "unique_words = tweets.filter_unique_voc_words()\n",
    "```\n",
    "\n",
    "The functions `filter_voc_words` and `filter_unique_voc_words` take a `method` attribute that can be either 'stemming' (default) or 'lemmatization'. This decides how the words are processed before being added to the vocabulary.\n",
    "\n",
    "One way to visualize the candidates' vocabulary is by creating word clouds. One of the exercises below asks you to do this using the full vocabulary. A solution is provided if you want to save time. The next exercise asks you to repeat the tasks using only superlatives and adjectives. The superlatives and adjectives are found with:\n",
    "\n",
    "```\n",
    "tweets = Tweets(filtered_tweets_clinton)\n",
    "adj = tweets.find_adjectives()\n",
    "sup = tweets.find_superlatives()\n",
    "```\n",
    "\n",
    "Plotting the word clouds for superlatives should require nothing more than changing the function name used in the previous word cloud exercise. The function `find_adjectives` returns a list of tuples, where each tuple has an adjective as the first item and the number of times the adjective appears in the tweets as the second item. The tuples are sorted by adjective frequency. Plotting the adjective word clouds requires converting this list of tuples into a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_MARKS = [\"!\", \".\", \"?\", \";\", \":\", \"?!\", \")\", \"(\", \"'\", \n",
    "                   '\"', \",\", \"-\", \"...\", \"…\", \"``\", \"`\", \"’\", \"“\", \n",
    "                   \"#\", \"&\", \"@\", \"—\", \"–\", \"”\", \"‘\"]\n",
    "# Clinton signs her tweets with her name, so we'll exclude her name\n",
    "# from the list of tokens. It should be automatically excluded since \n",
    "# we exclude proper nouns, but sometimes NLTK doesn't flag it.\n",
    "HILLARY = ['hillary', '-hillary', '-h', \"–hillary\", \"—hillary\"]\n",
    "STOPSET = stopwords.words('english') + HILLARY\n",
    "FORBIDDEN = [\"amp\"] + P_MARKS\n",
    "\n",
    "class Tweets(object):        \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokens = []\n",
    "        self._flagged_indices = []\n",
    "        self.tokenize()\n",
    "\n",
    "    def tokenize(self):\n",
    "        '''Tokenize all tweets and store the tokens in a list.'''\n",
    "        n_tokens = 0\n",
    "\n",
    "        # We use regex to detect links in tweets. Links are removed from the text.\n",
    "        url_regex = re.compile(r'https?:\\/\\/[a-zA-z0-9\\/#%\\.]+')\n",
    "\n",
    "        for tweet, hashtags, mentions in zip(self.df.text, self.df.hashtags, self.df.user_mentions):\n",
    "            hashtag_list, mention_list = [], []\n",
    "            # If there are hashtags or usernames mentioned in the tweet, they are\n",
    "            # split into a hashtags/mentions list.\n",
    "            if hashtags is not np.nan:\n",
    "                hashtag_list = [h for h in hashtags.lower().split(\", \")]\n",
    "            if mentions is not np.nan:\n",
    "                mention_list = [m for m in mentions.lower().split(\", \")]\n",
    "\n",
    "            # Remove links.\n",
    "            tweet = re.sub(url_regex, '', tweet)\n",
    "\n",
    "            # Tokenize.\n",
    "            tokens = word_tokenize(tweet)\n",
    "\n",
    "            # Separate words containing em dash, en dash, or forward slash. \n",
    "            for sep in [\"—\", \"–\", \"/\"]:\n",
    "                offset = 0\n",
    "                # List of words that need splitting and their position in the list of tokens.\n",
    "                split_words = [(i, word) for i, word in enumerate(tokens) if sep in word]\n",
    "                for i, word in split_words:\n",
    "                    i += offset\n",
    "                    # One of the words on either side of the separator could be an empty\n",
    "                    # string, in which case it's not saved in subwords.\n",
    "                    subwords = [subword for subword in word.split(sep) if subword != '']\n",
    "                    # The separated words are added to the list of tokens.\n",
    "                    tokens[i:i+1] = subwords\n",
    "                    # The offset is advanced such that next time we insert separated words \n",
    "                    # in the list of tokens they are inserted at the correct position.\n",
    "                    #     +1 if there's a word on both sides of the separator\n",
    "                    #      0 if there's a word only on one side of the separator \n",
    "                    #        (equivalent to simlpy removing the separator)\n",
    "                    #     -1 if the dash/slash does not separate words\n",
    "                    offset += (len(subwords) - 1)\n",
    "\n",
    "            # Store the indices of the words that are either hashtags or user mentions. \n",
    "            this_tweet_flagged_indices = [n_tokens + i \n",
    "                                          for i, w in enumerate(tokens) \n",
    "                                          if w.lower() in hashtag_list or w.lower() in mention_list]\n",
    "            self._flagged_indices += this_tweet_flagged_indices\n",
    "            n_tokens += len(tokens)\n",
    "            # Add the tweet tokens to the complete list of tokens.\n",
    "            self.tokens.extend(tokens)\n",
    "    \n",
    "    def tag_corpus(self):\n",
    "        '''Tag parts of speech (POS) and identify proper nouns.'''\n",
    "        \n",
    "        # If you're curious exactly what this function does,\n",
    "        # check out utils.py. I'll spare you the details here.\n",
    "        from utils import remove_numbers\n",
    "        \n",
    "        st_tags = st.tag(self.tokens)\n",
    "\n",
    "        # Words written in all caps are not always tagged\n",
    "        # correctly. Each word written in all caps that \n",
    "        # was not identified as a proper noun is changed \n",
    "        # to title case and the tagging is done again to\n",
    "        # to verify that it's indeed not a proper noun.\n",
    "        modified_corpus = False\n",
    "        for i, w in enumerate(st_tags):\n",
    "            if w[0].isupper() and w[1] == 'O':\n",
    "                self.tokens[i] = w[0].title()\n",
    "                modified_corpus = True\n",
    "        if modified_corpus:\n",
    "            st_tags = st.tag(self.tokens)\n",
    "            \n",
    "        pos_tags = nltk.pos_tag(self.tokens)\n",
    "        \n",
    "        # If a word is a hashtag or a username, don't include\n",
    "        # it in the list of tokens.\n",
    "        for index in reversed(self._flagged_indices):\n",
    "            del st_tags[index]\n",
    "            del pos_tags[index]\n",
    "            \n",
    "        st_tags = remove_numbers(st_tags)\n",
    "        pos_tags = remove_numbers(pos_tags)\n",
    "        \n",
    "        return st_tags, pos_tags\n",
    "    \n",
    "    def filter_voc_words(self, st_tags=None, pos_tags=None, method='stemming'):\n",
    "        '''Filter out words that are not indicative of vocabulary size:\n",
    "                - stopwords;\n",
    "                - very short words (<= 2 characters);\n",
    "                - hashtags;\n",
    "                - usernames;\n",
    "                - punctuation marks;\n",
    "                - proper nouns;\n",
    "                - numbers (spelled out or not).\n",
    "        '''\n",
    "        \n",
    "        # Translate complicated parts of speech to something easy,\n",
    "        # like simply verbs, nouns, adjectives, adverbs.\n",
    "        from utils import pos_translator\n",
    "        \n",
    "        if st_tags is None and pos_tags is None:\n",
    "            st_tags, pos_tags = self.tag_corpus()\n",
    "        elif len(st_tags) != len(pos_tags):\n",
    "            raise ValueError('Tag arguments st_tags and pos_tags must have equal length.')\n",
    "            \n",
    "        if method == 'lemmatization':\n",
    "            wn_lemmatizer = WordNetLemmatizer()\n",
    "            voc_words = [wn_lemmatizer.lemmatize(w[0].lower(), pos=pos_translator(p[1])) \n",
    "                         for w, p in zip(st_tags, pos_tags) \n",
    "                         if w[1] == 'O' and w[0].lower() not in STOPSET and\n",
    "                         len(w[0]) > 2 and not any(c.isdigit() for c in w[0]) and\n",
    "                         not any(f in w[0] for f in FORBIDDEN)]\n",
    "        elif method == 'stemming':\n",
    "            porter_stemmer = PorterStemmer()\n",
    "            voc_words = [porter_stemmer.stem(w[0].lower()) \n",
    "                         for w in st_tags\n",
    "                         if w[1] == 'O' and w[0].lower() not in STOPSET and\n",
    "                         len(w[0]) > 2 and not any(c.isdigit() for c in w[0]) and\n",
    "                         not any(f in w[0] for f in FORBIDDEN)]\n",
    "        else:\n",
    "            raise ValueError('Method %s not implemented. Choose either lemmatization or stemming.' %method)\n",
    "        return voc_words\n",
    "    \n",
    "    def filter_unique_voc_words(self, st_tags=None, pos_tags=None, voc_words=None, method='stemming'):\n",
    "        '''Remove repeating words from the vocabulary.'''\n",
    "        if not voc_words:\n",
    "            voc_words = self.filter_voc_words(st_tags=st_tags, pos_tags=pos_tags, method=method)\n",
    "        unique_voc_words = set(voc_words)\n",
    "        return unique_voc_words\n",
    "    \n",
    "    def find_superlatives(self, st_tags=None, pos_tags=None, filter_by=['adj', 'adv']):\n",
    "        '''Find superlative adjectives and/or adverbs.'''\n",
    "        if st_tags is None and pos_tags is None:\n",
    "            st_tags, pos_tags = self.tag_corpus()\n",
    "        elif len(st_tags) != len(pos_tags):\n",
    "            raise ValueError('Tag arguments st_tags and pos_tags must have equal length.')\n",
    "            \n",
    "        if isinstance(filter_by, str):\n",
    "            filter_by = [filter_by]\n",
    "        elif not isinstance(filter_by, list):\n",
    "            raise TypeError('Filter should be either single POS (string/list) or list of POS.')\n",
    "            \n",
    "        # Selection can be done based on superlative adjectives,\n",
    "        # superlative adverbs, or both adjectives and adverbs.\n",
    "        pos_filter = []\n",
    "        if 'adj' in filter_by:\n",
    "            pos_filter.append('JJS')\n",
    "        if 'adv' in filter_by:\n",
    "            pos_filter.append('RBS')\n",
    "        if 'JJS' not in pos_filter and 'adj' not in pos_filter:\n",
    "            raise ValueError('Cannot filter based on %s. Options are adj or/and adv.' % str(filter_by) )\n",
    "\n",
    "        superlatives = [w[0].lower() for w, p in zip(st_tags, pos_tags)\n",
    "                        if w[1] == 'O' and w[0].lower() not in STOPSET and\n",
    "                        len(w[0]) > 2 and not any(c.isdigit() for c in w[0]) and\n",
    "                        not any(f in w[0] for f in FORBIDDEN) and\n",
    "                        p[1] in pos_filter]\n",
    "        return superlatives\n",
    "    \n",
    "    def find_adjectives(self, st_tags=None, pos_tags=None):\n",
    "        '''Find the adjectives in tweets.'''\n",
    "        import operator\n",
    "        from utils import pos_translator\n",
    "        \n",
    "        if st_tags is None and pos_tags is None:\n",
    "            st_tags, pos_tags = self.tag_corpus()\n",
    "        elif len(st_tags) != len(pos_tags):\n",
    "            raise ValueError('Tag arguments st_tags and pos_tags must have equal length.')\n",
    "        \n",
    "        wn_lemmatizer = WordNetLemmatizer()\n",
    "        adjectives = [wn_lemmatizer.lemmatize(w[0].lower(), pos=pos_translator(p[1])) \n",
    "                  for w, p in zip(st_tags, pos_tags)\n",
    "                  if w[1] == 'O' and w[0].lower() not in STOPSET and\n",
    "                  len(w[0]) > 2 and not any(c.isdigit() for c in w[0]) and\n",
    "                  not any(f in w[0].lower() for f in FORBIDDEN) and\n",
    "                  (pos_translator(p[1]) == 'a')]\n",
    "        unique_adj = set(adjectives)\n",
    "        adj = {}\n",
    "        for a in unique_adj:\n",
    "            adj[a] = adjectives.count(a)\n",
    "        sorted_adj = sorted(adj.items(), key=operator.itemgetter(1))\n",
    "        return sorted_adj\n",
    "    \n",
    "    def analyze_sentiment(self, emo_dict, st_tags=None, pos_tags=None, \n",
    "                          voc_words=None, method='stemming'):\n",
    "        if st_tags is None and pos_tags is None:\n",
    "            st_tags, pos_tags = self.tag_corpus()\n",
    "        elif len(st_tags) != len(pos_tags):\n",
    "            raise ValueError('Tag arguments st_tags and pos_tags must have equal length.')\n",
    "        \n",
    "        if voc_words is None:\n",
    "            voc_words = self.filter_voc_words(st_tags=st_tags, pos_tags=pos_tags, method=method)\n",
    "        n_words = len(voc_words)\n",
    "        \n",
    "        sentiment = {'anger': 0,\n",
    "                    'anticipation': 0,\n",
    "                    'disgust': 0,\n",
    "                    'fear': 0,\n",
    "                    'joy': 0,\n",
    "                    'negative': 0,\n",
    "                    'positive': 0,\n",
    "                    'sadness': 0,\n",
    "                    'surprise': 0,\n",
    "                    'trust': 0}\n",
    "        found_words = 0\n",
    "        for word in set(voc_words):\n",
    "            if word not in emo_dict:\n",
    "                continue\n",
    "            found_words += voc_words.count(word)\n",
    "            for s in sentiment:\n",
    "                sentiment[s] += voc_words.count(word) * emo_dict[word][s]\n",
    "        \n",
    "        # Normalize by the number of words found in the file.\n",
    "        # Repetitions should be included.\n",
    "        for s in sentiment:\n",
    "            sentiment[s] = sentiment[s] * 100. / found_words\n",
    "        \n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*EXERCISE: Compare Clinton's and Trump's vocabulary size. Which of them has a more diverse vocabulary?*\n",
    "\n",
    "Running the cell below will show you a possible solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load voc_size_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*EXERCISE: Write a function to create wordclouds based on Clinton's and Trump's tweets. You can use [A. M&uuml;ller's wordcloud generator](https://github.com/amueller/word_cloud). Some examples are available in the `word_cloud` README file.*\n",
    "\n",
    "Running the cell below will show you a possible solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load wordcloud_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*EXERCISE: Find the superlatives used by Clinton and Trump. How good is the code at detecting superlatives? Make wordclouds using only the superlatives. Try the same exercise using adjectives.* \n",
    "\n",
    "The solution is very similar to the one for the previous exercise, so I'll leave this one for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's look at the difference in adjective usage between Trump and Clinton. Is one candidate using more negative adjectives than the other? We'll only consider adjectives that appear in both candidates' vocabulary, and we'll calculate the frequency ratio with which each adjective is used by one candidate compared to the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_trump = Tweets(filtered_tweets_trump)\n",
    "trump_adj = tweets_trump.find_adjectives()\n",
    "all_trump_adj = [x[0] for x in trump_adj]\n",
    "n_adj_trump = np.sum([x[1] for x in trump_adj])\n",
    "\n",
    "tweets_clinton = Tweets(filtered_tweets_clinton)\n",
    "clinton_adj = tweets_clinton.find_adjectives()\n",
    "all_clinton_adj = [x[0] for x in clinton_adj]\n",
    "n_adj_clinton = np.sum([x[1] for x in clinton_adj])\n",
    "\n",
    "adj_freq = {}\n",
    "all_adj = set(all_trump_adj + all_clinton_adj)\n",
    "for adj in all_adj:\n",
    "    if adj not in all_clinton_adj or adj not in all_trump_adj:\n",
    "        continue\n",
    "    f_clinton = [x[1] / n_adj_clinton * 100. for x in clinton_adj if x[0] == adj][0]\n",
    "    f_trump = [x[1] / n_adj_trump * 100. for x in trump_adj if x[0] == adj][0]\n",
    "    adj_freq[adj] = [f_clinton, f_trump]\n",
    "\n",
    "# Ignore adjectives that are used with a frequency ratio lower than 5.\n",
    "min_freq_ratio = 5\n",
    "\n",
    "# Depending on how much data you downloaded, there are quite a few adjectives \n",
    "# with frequency ratio >= 5. So, when plotting, we'll only show the top 20 \n",
    "# adjectives (i.e., top 20 adjectives with the largest Trump-to-Clinton frequency\n",
    "# ratio, and top 20 with the largest Clinton-to-Trump frequency ratio).\n",
    "n_top_adj = 20\n",
    "adj_ratio = [(adj, n[0]/n[1]) for adj, n in adj_freq.items() if n[0]/n[1] >= min_freq_ratio]\n",
    "sorted_adj_ratio_clinton = sorted(adj_ratio, key=lambda x: x[1])[-n_top_adj:]\n",
    "\n",
    "adj_ratio = [(adj, n[1]/n[0]) for adj, n in adj_freq.items() if n[1]/n[0] >= min_freq_ratio]\n",
    "sorted_adj_ratio_trump = sorted(adj_ratio, key=lambda x: x[1])[-n_top_adj:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the plotting... I'm using a bar plot here, but you can play with other ways of visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams.update({'xtick.major.size': 10})\n",
    "plt.rcParams.update({'ytick.major.size': 10})\n",
    "pfont = {'fontname': 'Palatino Linotype'}\n",
    "\n",
    "fig = plt.figure(figsize=(12,16), frameon=False)\n",
    "plt.axis('off')\n",
    "\n",
    "bar_pos_trump = np.arange(len(sorted_adj_ratio_trump))\n",
    "bar_pos_clinton = len(sorted_adj_ratio_trump) + np.arange(len(sorted_adj_ratio_clinton))\n",
    "bar_width = 0.7\n",
    "\n",
    "plt.barh(bar_pos_clinton, \n",
    "        [x[1] for x in sorted_adj_ratio_clinton], \n",
    "        color='navy', height=bar_width)\n",
    "plt.barh(bar_pos_trump,\n",
    "        [-x[1] for x in sorted_adj_ratio_trump],\n",
    "        color='firebrick', height=bar_width)\n",
    "tc = 'slategray'\n",
    "for i, adj in zip(bar_pos_trump, sorted_adj_ratio_trump):\n",
    "    tloc = -100\n",
    "    talign = 'right'\n",
    "    offset = abs(tloc)/30*np.sign(tloc)\n",
    "    lstart = np.max(adj[1]) * np.sign(tloc) + 6*offset\n",
    "    nmax_label = lstart\n",
    "    nmin_label = np.min(adj[1])*np.sign(offset) \n",
    "    lend = tloc - offset\n",
    "    plt.text(tloc, i, adj[0].upper(), va='bottom', ha=talign, color=tc, **pfont)\n",
    "    plt.text(np.max(adj[1]) * np.sign(tloc) + offset, i, \"%.1f\" % np.max(adj[1]), \n",
    "             ha=talign, va='bottom', color=tc, **pfont)\n",
    "    plt.plot([lstart, lend], [i+bar_width/2, i+bar_width/2], \n",
    "              linestyle='dotted', c='k', alpha=0.3, linewidth=1)\n",
    "for i, adj in zip(bar_pos_clinton, sorted_adj_ratio_clinton):\n",
    "    tloc = 100\n",
    "    talign = 'left'\n",
    "    offset = abs(tloc)/30*np.sign(tloc)\n",
    "    lstart = np.max(adj[1]) * np.sign(tloc) + 6*offset\n",
    "    nmax_label = lstart\n",
    "    nmin_label = np.min(adj[1])*np.sign(offset) \n",
    "    lend = tloc - offset\n",
    "    plt.text(tloc, i, adj[0].upper(), va='bottom', ha=talign, color=tc, **pfont)\n",
    "    plt.text(np.max(adj[1]) * np.sign(tloc) + offset, i, \"%.1f\" % np.max(adj[1]), \n",
    "             ha=talign, va='bottom', color=tc, **pfont)\n",
    "    plt.plot([lstart, lend], [i+bar_width/2, i+bar_width/2], \n",
    "              linestyle='dotted', c='k', alpha=0.3, linewidth=1)\n",
    "\n",
    "plt.xlim([-abs(tloc)*1.25,abs(tloc)*1.25])\n",
    "plt.text(-8, np.max(bar_pos_clinton)+2.0, \"TRUMP'S ADJECTIVES\", ha='right', fontsize=20, alpha=0.8, **pfont)\n",
    "plt.text(8, np.max(bar_pos_clinton)+2.0, \"CLINTON'S ADJECTIVES\", ha='left', fontsize=20, alpha=0.8, **pfont)\n",
    "plt.text(0, -4.0, \n",
    "         \"\"\"Adjectives appearing in both Trump's and Clinton's tweets that are used by one of them\n",
    "at least five times more frequently than by the other. Numbers indicate how often a candidate \n",
    "uses a word compared to their opponent.\"\"\", fontsize=18, alpha=0.8, \n",
    "         ha='center', style='italic', **pfont)\n",
    "\n",
    "plt.savefig(root_dir + '/figs/pyladies_adj_differences.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `data/NRC-Emotion-Lexicon-v0.92` directory, there is a file named `NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt`. This file contains a long list of words and the sentiments associated with them. Take a moment to understand the content of the file. We'll analyze the sentiments in Trump's and Clinton's speech by looking up the words they tweeted in the file, and summing up the 'scores' for each sentiment. For example, for the sentence\n",
    "\n",
    "\"I enjoyed a delicious ice cream.\"\n",
    "\n",
    "the words 'enjoy', 'delicious', and 'cream' can be found in the file. Their scores are\n",
    "\n",
    " | cream | delicious | enjoy\n",
    " ---|---|---|---\n",
    "ANGER | 0 | 0 | 0 \n",
    "ANTICIPATION | 0 | 0 | 1\n",
    "DISGUST | 0 | 0 | 0\n",
    "FEAR | 0 | 0 | 0\n",
    "JOY | 1 | 1 | 1\n",
    "NEGATIVE | 0 | 0 | 0\n",
    "POSITIVE | 1 | 1 | 1\n",
    "SADNESS | 0 | 0 | 0\n",
    "SURPRISE | 1 | 0 | 0\n",
    "TRUST | 0 | 0 | 1\n",
    "\n",
    "so the sentence would score 1 for anticipation, 3 for joy, 3 for positive, 1 for suprise, 1 for trust, and 0 for anger, disgust, fear, negative, and sadness.\n",
    "\n",
    "We'll do a similar exercise for Clinton's and Trump's tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the look up, we'll first copy the content of the sentiment file into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make emotion dictionary\n",
    "emo_file = root_dir + \"/data/NRC-Emotion-Lexicon-v0.92/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\" \n",
    "emo_dict = {}\n",
    "with open(emo_file, 'r') as f:\n",
    "    # Skip the header (lines 0-45).\n",
    "    for emo_line in f.readlines()[46:]:\n",
    "        emo_info = emo_line.strip().split()\n",
    "        word = emo_info[0]\n",
    "        emo = emo_info[1]\n",
    "        emo_score = int(emo_info[2])\n",
    "        if word not in emo_dict:\n",
    "            emo_dict[word] = {}\n",
    "        emo_dict[word][emo] = emo_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the sentiment dictionary, we can go ahead and classify each candidate's words. We'll use the function `analyze_sentiment` in the `Tweets` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_trump = tweets_trump.analyze_sentiment(emo_dict, method='lemmatization')\n",
    "sentiment_clinton = tweets_clinton.analyze_sentiment(emo_dict, method='lemmatization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting, we sort the sentiment scores alphabetically. We'll also move 'positive' and 'negative' at the top of the sentiment list, since these are not really sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def move_element(l, w, p):\n",
    "    v = [x for x in l if x[0] == w][0]\n",
    "    l = [x for x in l if x[0] != w]\n",
    "    l.insert(p, v)\n",
    "    return l\n",
    "\n",
    "def sort_sentiment(sentiment):\n",
    "    sorted_sentiment = sorted(sentiment.items(), key=lambda x: x[0], reverse=True)\n",
    "    sorted_sentiment = move_element(sorted_sentiment, \"positive\", 0)\n",
    "    sorted_sentiment = move_element(sorted_sentiment, \"negative\", 0)\n",
    "    return sorted_sentiment\n",
    "\n",
    "sorted_sentiment_trump = sort_sentiment(sentiment_trump)\n",
    "sorted_sentiment_clinton = sort_sentiment(sentiment_clinton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we plot the results, again using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams.update({'xtick.major.size': 10})\n",
    "plt.rcParams.update({'ytick.major.size': 10})\n",
    "pfont = {'fontname': 'Palatino Linotype'}\n",
    "tc = 'slategray'\n",
    "\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "plt.axis('off')\n",
    "\n",
    "bar_pos = np.arange(10)\n",
    "bar_width = 0.7\n",
    "\n",
    "clinton_keys = [x[0] for x in sorted_sentiment_clinton]\n",
    "trump_keys = [x[0] for x in sorted_sentiment_trump]\n",
    "clinton_vals = [x[1] for x in sorted_sentiment_clinton]\n",
    "trump_vals = [-x[1] for x in sorted_sentiment_trump]\n",
    "\n",
    "plt.barh(bar_pos, clinton_vals, color='navy')\n",
    "plt.barh(bar_pos, trump_vals, color='firebrick')\n",
    "\n",
    "for pos, emo in zip(bar_pos, clinton_keys):\n",
    "    if sentiment_clinton[emo] > sentiment_trump[emo]:\n",
    "        tw_right, tw_left = 'black', 'light'\n",
    "        tc_right, tc_left = 'dimgray', tc\n",
    "        tal_right, tal_left = 1., 0.75\n",
    "    elif sentiment_clinton[emo] < sentiment_trump[emo]:\n",
    "        tw_right, tw_left = 'light', 'black'\n",
    "        tc_right, tc_left = tc, 'dimgray'\n",
    "        tal_right, tal_left = 0.75, 1.0\n",
    "    else:\n",
    "        tw_right, tw_left = 'black', 'black'\n",
    "        tc_right, tc_left = 'dimgray', 'dimgray'\n",
    "        tal_right, tal_left = 1.0, 1.0\n",
    "    plt.text(24, pos+0.1, emo.upper(), va='bottom', ha='right', \n",
    "             color=tc_right, fontsize=18, weight=tw_right, alpha=tal_right, **pfont)\n",
    "    plt.text(-24, pos+0.1, emo.upper(), va='bottom', ha='left', \n",
    "             color=tc_left, fontsize=18, weight=tw_left, alpha=tal_left, **pfont)\n",
    "    plt.text(0.5, pos+0.1, \"%.1f\" % sentiment_clinton[emo], weight='black', va='bottom', \n",
    "             ha='left', color='white', fontsize=20, **pfont)\n",
    "    plt.text(-0.5, pos+0.1, \"%.1f\" % sentiment_trump[emo], weight='black', va='bottom', \n",
    "             ha='right', color='white', fontsize=20, **pfont)\n",
    "\n",
    "plt.text(1, 10.4, \"CLINTON'S TWEETS\", fontsize=20, ha='left', alpha=0.8, weight='heavy', **pfont)\n",
    "plt.text(-1, 10.4, \"TRUMP'S TWEETS\", fontsize=20, ha='right', alpha=0.8, weight='heavy', **pfont)\n",
    "plt.text(0, -1.1, \"\"\"Sentiment analysis based on Trump's and Clinton's tweets. Numbers represent fraction of candidate's words classified in a sentiment category. \n",
    "Sentiments more frequent in a certain candidate's tweets are written in bold face—Trump's on the left and Clinton's on the right.\"\"\", \n",
    "         fontsize=22, alpha=0.8, \n",
    "         ha='center', style='italic', **pfont)\n",
    "\n",
    "plt.xlim([-20,20])\n",
    "\n",
    "plt.savefig(root_dir + \"/figs/pyladies_sentiment_analysis.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
