{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a comparative analysis of [Hillary Clinton's](https://twitter.com/HillaryClinton) and [Donald Trump's](https://twitter.com/realDonaldTrump) tweets. The repo contains the code and a couple of images used for the data visualization. \n",
    "\n",
    "We will use the Twitter API to download the data. Because the API only allows one to download a user's last 3200 tweets, your results might be a little different than mine.\n",
    "\n",
    "Running this analysis requires Python 3 and the following packages:\n",
    "\n",
    "* yaml\n",
    "* numpy\n",
    "* matplotlib\n",
    "* pandas\n",
    "* tweepy\n",
    "* bokeh\n",
    "* nltk\n",
    "\n",
    "If you're willing to tweak the code a bit more, than Python 2 should be fine too.\n",
    "\n",
    "Downloading the data requires a Twitter account and a mobile phone number associated with the account. The phone number can be added in `Profile and settings -> Settings -> Mobile`: https://twitter.com/settings/add_phone?edit_phone=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial consists of two parts. The first part should be easy to complete. The second part is slightly more difficult and has several exercises that you can work on (solutions are provided though). \n",
    "\n",
    "The second part requires the Stanford Named Entity Recognition Tagger. This blog post explains how to get it to work: http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages.\n",
    "\n",
    "The second part also requires an emotion lexicon. I used the one here: http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "\n",
    "1. [Download Clinton's and Trump's Tweets](Download Tweets.ipynb)\n",
    "2. [Vocabulary Size and Sentiment Analysis](Vocabulary Size and Sentiment Analysis.ipynb)\n",
    "\n",
    "NLTK is needed for the second part. At the end of this notebook, there is a brief introduction to the NLTK methods that will be used in the tutorial. This is very basic, and those who worked with NLTK before can probably skip all of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suggest the following directory structure inside your top level directory:\n",
    "\n",
    "```\n",
    ".\n",
    "├── code\n",
    "├── data\n",
    "│   └── NRC-Emotion-Lexicon-v0.92\n",
    "├── figs\n",
    "└── tutorial\n",
    "    ├── img\n",
    "    └── notebooks\n",
    "```\n",
    "\n",
    "If you clone the Git repository, then most of these directories will be created automatically. Exceptions are the `figs` directory (because all the files can be generated by running the notebooks) and the CSV files in the `data` directory that contain the tweets info (because, per the Twitter Developer Agreement, one is not allowed to share their Twitter data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK](http://www.nltk.org/) in a Python package for natural language processing. The main NLTK methods that we will be using for this tutorial are `word_tokenize`, `pos_tag`, and `stopwords`. Additionally, we will also use the tagging, stemming, and lemmatization classes `StanfordNERTagger`, `PorterStemmer`, and `WordNetLemmatizer`, respectively.\n",
    "\n",
    "The examples below show simple uses of these methods, which should be enough to write/understand the code in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'very', 'boring', 'sentence', 'for', 'the', 'PyLadies', 'tutorial', '.'] \n",
      "\n",
      "['“Gangsta”', 'is', 'hardly', 'a', 'new', 'word', ';', 'in', 'fact', ',', 'it', \"'s\", 'at', 'least', 'two', 'decades', 'old', '.', 'But', 'a', 'new', 'take', 'on', 'someone', 'who', 'aspires', 'to', 'the', 'gangsta', 'style', ',', 'but', 'fails', 'miserably', ',', 'is', 'a', '“wanksta.”']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a text.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text1 = \"This is a very boring sentence for the PyLadies tutorial.\"\n",
    "tokenized_text1 = word_tokenize(text1)\n",
    "print(tokenized_text1, \"\\n\")\n",
    "\n",
    "# from http://examples.yourdictionary.com/20-examples-of-slang-language.html\n",
    "text2 = \"\"\"“Gangsta” is hardly a new word; in fact, it's at least two decades old. \n",
    "           But a new take on someone who aspires to the gangsta style, but fails miserably, \n",
    "           is a “wanksta.”\"\"\"\n",
    "tokenized_text2 = word_tokenize(text2)\n",
    "print(tokenized_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('very', 'RB'), ('boring', 'JJ'), ('sentence', 'NN'), ('for', 'IN'), ('the', 'DT'), ('PyLadies', 'NNP'), ('tutorial', 'NN'), ('.', '.')] \n",
      "\n",
      "[('“Gangsta”', 'NN'), ('is', 'VBZ'), ('hardly', 'RB'), ('a', 'DT'), ('new', 'JJ'), ('word', 'NN'), (';', ':'), ('in', 'IN'), ('fact', 'NN'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('at', 'IN'), ('least', 'JJS'), ('two', 'CD'), ('decades', 'NNS'), ('old', 'JJ'), ('.', '.'), ('But', 'CC'), ('a', 'DT'), ('new', 'JJ'), ('take', 'NN'), ('on', 'IN'), ('someone', 'NN'), ('who', 'WP'), ('aspires', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('gangsta', 'NN'), ('style', 'NN'), (',', ','), ('but', 'CC'), ('fails', 'VBZ'), ('miserably', 'RB'), (',', ','), ('is', 'VBZ'), ('a', 'DT'), ('“wanksta.”', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Tag parts of speech in a text.\n",
    "#\n",
    "# See this list for possible tags:\n",
    "# http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "import nltk\n",
    "\n",
    "print(nltk.pos_tag(tokenized_text1), \"\\n\")\n",
    "print(nltk.pos_tag(tokenized_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Crazy', 'O'), ('Maureen', 'PERSON'), ('Dowd', 'PERSON'), (',', 'O'), ('the', 'O'), ('wacky', 'O'), ('columnist', 'O'), ('for', 'O'), ('the', 'O'), ('failing', 'O'), ('@', 'O'), ('nytimes', 'O'), (',', 'O'), ('pretends', 'O'), ('she', 'O'), ('knows', 'O'), ('me', 'O'), ('well', 'O'), ('--', 'O'), ('wrong', 'O'), ('!', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Tag a text using the Stanford NER tagger.\n",
    "import os\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# Change the paths to point to the directory where you downloaded the Stanford tagger.\n",
    "os.environ['STANFORD_MODELS'] = \"/Users/gogrean/code/stanford-ner-2014-06-16/classifiers\"\n",
    "os.environ['CLASSPATH'] = \"/Users/gogrean/code/stanford-ner-2014-06-16\"\n",
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "# from Trump's tweets\n",
    "text = \"\"\"Crazy Maureen Dowd, the wacky columnist for the failing \n",
    "          @nytimes, pretends she knows me well--wrong!\"\"\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "\n",
    "print(st.tag(tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are two ways of processing text that identify inflected words and words with a common stem, and reduce them to a common form. The examples below give a general idea of how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish fish fish\n"
     ]
    }
   ],
   "source": [
    "# some word examples from Wikipedia\n",
    "# https://en.wikipedia.org/wiki/Stemming#Examples\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# sometimes stemming returns actual words\n",
    "print( porter_stemmer.stem('fish'), porter_stemmer.stem('fishing'), porter_stemmer.stem('fishes') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argu argu argu\n"
     ]
    }
   ],
   "source": [
    "# other times it doesn't\n",
    "print( porter_stemmer.stem('argue'), porter_stemmer.stem('argued'), porter_stemmer.stem('arguing') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog dog\n"
     ]
    }
   ],
   "source": [
    "# sometimes the results and not necessarily what we want\n",
    "print( porter_stemmer.stem('dog'), porter_stemmer.stem('dogs'), porter_stemmer.stem('dogged') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish fishing fish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print( wn_lemmatizer.lemmatize('fish'), wn_lemmatizer.lemmatize('fishing'), wn_lemmatizer.lemmatize('fishes') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argue argued arguing\n"
     ]
    }
   ],
   "source": [
    "# this doesn't work great\n",
    "print( wn_lemmatizer.lemmatize('argue'), wn_lemmatizer.lemmatize('argued'), wn_lemmatizer.lemmatize('arguing') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argue argue argue\n"
     ]
    }
   ],
   "source": [
    "# it works much better if we provide the part of speech\n",
    "print( wn_lemmatizer.lemmatize('argue', 'v'), wn_lemmatizer.lemmatize('argued', 'v'), \n",
    "       wn_lemmatizer.lemmatize('arguing', 'v') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog dogged\n"
     ]
    }
   ],
   "source": [
    "print( wn_lemmatizer.lemmatize('dog'), wn_lemmatizer.lemmatize('dogs'), wn_lemmatizer.lemmatize('dogged') )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
